<html>
<head>
<title>cluster.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #8c8c8c; font-style: italic;}
.s1 { color: #080808;}
.s2 { color: #0033b3;}
.s3 { color: #067d17;}
.s4 { color: #1750eb;}
.s5 { color: #0037a6;}
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
cluster.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">#%% 
# Import necessary libraries</span>
<span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">matplotlib.pyplot </span><span class="s2">as </span><span class="s1">plt</span>
<span class="s2">import </span><span class="s1">seaborn </span><span class="s2">as </span><span class="s1">sns</span>
<span class="s2">from </span><span class="s1">sklearn.impute </span><span class="s2">import </span><span class="s1">SimpleImputer</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">StandardScaler</span>
<span class="s2">from </span><span class="s1">sklearn.cluster </span><span class="s2">import </span><span class="s1">KMeans</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">silhouette_score</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">train_test_split</span>
<span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">RandomForestRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">LinearRegression</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">mean_squared_error, r2_score</span>
<span class="s2">from </span><span class="s1">scipy.stats.mstats </span><span class="s2">import </span><span class="s1">winsorize</span>
<span class="s2">from </span><span class="s1">sklearn.decomposition </span><span class="s2">import </span><span class="s1">PCA</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">cross_val_score</span>
<span class="s0">#%% md 
</span><span class="s1">## 1. Data Source and Preparation 
</span><span class="s0">#%% md 
</span><span class="s1">#### Loading the Dataset 
- The dataset is loaded from a CSV file for analysis. 
- **Metadata columns** (used for row identification) are retained. 
 
#### Selecting Relevant Features 
- **Health outcome columns** are dropped, except for **Premature Death**, which is used as the target variable. 
- **Additional measures** are removed as per the assignment requirements. 
- Columns that do not contribute meaningful information to the analysis are also dropped. 
 
#### Data Type Conversion 
- **Raw_value columns** are converted to **numerical format** to facilitate calculations in **Exploratory Data Analysis (EDA)** and further modeling. 
 
</span><span class="s0">#%% 
# Load the dataset while skipping the first descriptive row</span>
<span class="s1">file_path = </span><span class="s3">'analytic_data2024.csv'</span>
<span class="s1">df = pd.read_csv(file_path,dtype=str, header=[</span><span class="s4">1</span><span class="s1">])</span>
<span class="s1">print(</span><span class="s3">&quot;Data shape: &quot;</span><span class="s1">, df.shape)</span>
<span class="s0">#%% 
# List of metadata columns to retain</span>
<span class="s1">metadata_columns = [</span><span class="s3">&quot;statecode&quot;</span><span class="s1">, </span><span class="s3">&quot;countycode&quot;</span><span class="s1">, </span><span class="s3">&quot;fipscode&quot;</span><span class="s1">, </span><span class="s3">&quot;state&quot;</span><span class="s1">, </span><span class="s3">&quot;county&quot;</span><span class="s1">, </span><span class="s3">&quot;year&quot;</span><span class="s1">, </span><span class="s3">&quot;county_clustered&quot;</span><span class="s1">]</span>

<span class="s0"># List of Health Outcomes to exclude except for Premature Death</span>
<span class="s1">health_outcomes = [</span>
    <span class="s3">&quot;v002_rawvalue&quot;</span><span class="s1">, </span><span class="s0"># Poor or Fair Health</span>
    <span class="s3">&quot;v036_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Poor Physical Health Days</span>
    <span class="s3">&quot;v042_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Poor Mental Health Days</span>
    <span class="s3">&quot;v037_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Low Birthweight</span>
<span class="s1">]</span>

<span class="s0"># List of Additional Measures to exclude</span>
<span class="s1">additional_measures = [</span>
    <span class="s3">&quot;v147_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Life Expectancy</span>
    <span class="s3">&quot;v127_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Premature Age-Adjusted Mortality</span>
    <span class="s3">&quot;v128_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Child Mortality</span>
    <span class="s3">&quot;v129_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Infant Mortality</span>
    <span class="s3">&quot;v144_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Frequent Physical Distress</span>
    <span class="s3">&quot;v145_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Frequent Mental Distress</span>
    <span class="s3">&quot;v060_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Diabetes Prevalence</span>
    <span class="s3">&quot;v061_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># HIV Prevalence</span>
    <span class="s3">&quot;v139_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Food Insecurity</span>
    <span class="s3">&quot;v083_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Limited Access to Healthy Foods</span>
    <span class="s3">&quot;v138_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Drug Overdose Deaths</span>
    <span class="s3">&quot;v143_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Insufficient Sleep</span>
    <span class="s3">&quot;v003_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Uninsured Adults</span>
    <span class="s3">&quot;v122_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Uninsured Children</span>
    <span class="s3">&quot;v131_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Other Primary Care Providers</span>
    <span class="s3">&quot;v021_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># High School Graduation</span>
    <span class="s3">&quot;v149_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Disconnected Youth</span>
    <span class="s3">&quot;v159_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Reading Scores</span>
    <span class="s3">&quot;v160_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Math Scores</span>
    <span class="s3">&quot;v167_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># School Segregation</span>
    <span class="s3">&quot;v169_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># School Funding Adequacy</span>
    <span class="s3">&quot;v151_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Gender Pay Gap</span>
    <span class="s3">&quot;v063_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Median Household Income</span>
    <span class="s3">&quot;v170_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Living Wage</span>
    <span class="s3">&quot;v065_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Children Eligible for Free or Reduced Price Lunch</span>
    <span class="s3">&quot;v141_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Residential Segregation - Black/White</span>
    <span class="s3">&quot;v171_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Child Care Cost Burden</span>
    <span class="s3">&quot;v172_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Child Care Centers</span>
    <span class="s3">&quot;v015_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Homicides</span>
    <span class="s3">&quot;v161_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Suicides</span>
    <span class="s3">&quot;v148_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Firearm Fatalities</span>
    <span class="s3">&quot;v039_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Motor Vehicle Crash Deaths</span>
    <span class="s3">&quot;v158_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Juvenile Arrests</span>
    <span class="s3">&quot;v177_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Voter Turnout</span>
    <span class="s3">&quot;v178_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Census Participation</span>
    <span class="s3">&quot;v156_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Traffic Volume</span>
    <span class="s3">&quot;v153_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Homeownership</span>
    <span class="s3">&quot;v154_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Severe Housing Cost Burden</span>
    <span class="s3">&quot;v166_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Broadband Access</span>
    <span class="s3">&quot;v051_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># Population</span>
    <span class="s3">&quot;v052_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># % Below 18 Years of Age</span>
    <span class="s3">&quot;v053_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># % 65 and Older</span>
    <span class="s3">&quot;v054_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># % Non-Hispanic Black</span>
    <span class="s3">&quot;v055_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># % American Indian or Alaska Native</span>
    <span class="s3">&quot;v081_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># % Asian</span>
    <span class="s3">&quot;v080_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># % Native Hawaiian or Other Pacific Islander</span>
    <span class="s3">&quot;v056_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># % Hispanic</span>
    <span class="s3">&quot;v126_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># % Non-Hispanic White</span>
    <span class="s3">&quot;v059_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># % Not Proficient in English</span>
    <span class="s3">&quot;v057_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># % Female</span>
    <span class="s3">&quot;v058_rawvalue&quot;</span><span class="s1">,  </span><span class="s0"># % Rural</span>
<span class="s1">]</span>

<span class="s0">#Select columns that contain &quot;rawvalue&quot; but exclude health outcomes and additional measures</span>
<span class="s1">rawvalue_columns = [col </span><span class="s2">for </span><span class="s1">col </span><span class="s2">in </span><span class="s1">df.columns </span><span class="s2">if </span><span class="s3">&quot;rawvalue&quot; </span><span class="s2">in </span><span class="s1">col]</span>
<span class="s1">selected_features = [col </span><span class="s2">for </span><span class="s1">col </span><span class="s2">in </span><span class="s1">rawvalue_columns </span><span class="s2">if </span><span class="s1">col </span><span class="s2">not in </span><span class="s1">health_outcomes </span><span class="s2">and </span><span class="s1">col </span><span class="s2">not in </span><span class="s1">additional_measures]</span>

<span class="s0"># New DataFrame with only selected features and metadata columns</span>
<span class="s1">df_filtered = df[metadata_columns + selected_features]</span>

<span class="s1">df_filtered.head()</span>
<span class="s0">#%% 
# Unique numbers of value in each column</span>
<span class="s1">unique_counts = df_filtered.nunique()</span>
<span class="s1">print(unique_counts)</span>
<span class="s0">#%% 
# Check data type of each column</span>
<span class="s1">print(df_filtered.info())</span>
<span class="s0">#%% 
# Drop 'year' column because it contains only 2024</span>
<span class="s1">df_filtered = df_filtered.drop(columns=[</span><span class="s3">'year'</span><span class="s1">])</span>
<span class="s0">#%% 
# Drop rows where 'county_clustered' is missing (only keeping county-level data)</span>
<span class="s1">data = df_filtered.dropna(subset=[</span><span class="s3">'county_clustered'</span><span class="s1">])</span>
<span class="s0">#%% 
# Confirm that only county-level rows are kept</span>
<span class="s1">print(data[</span><span class="s3">'county_clustered'</span><span class="s1">].isnull().sum())</span>
<span class="s0">#%% 
# Change the data type of &quot;rawvalue&quot; column to numerical</span>
<span class="s1">data = data.copy()</span>
<span class="s1">raw_value_columns = [col </span><span class="s2">for </span><span class="s1">col </span><span class="s2">in </span><span class="s1">data.columns </span><span class="s2">if </span><span class="s3">'_rawvalue' </span><span class="s2">in </span><span class="s1">col]</span>
<span class="s1">data[raw_value_columns] = data[raw_value_columns].apply(pd.to_numeric, errors=</span><span class="s3">'coerce'</span><span class="s1">)</span>
<span class="s1">print(data.dtypes)</span>
<span class="s0">#%% 
# Mapping Raw Value Columns to Feature Names</span>
<span class="s1">feature_mapping = {</span>
    <span class="s3">&quot;v001_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Premature Death&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v009_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Adult Smoking&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v011_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Adult Obesity&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v133_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Food Environment Index&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v070_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Physical Inactivity&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v132_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Access to Exercise Opportunities&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v049_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Excessive Drinking&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v134_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Alcohol-Impaired Driving Deaths&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v045_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Sexually Transmitted Infections&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v014_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Teen Births&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v085_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Uninsured&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v004_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Primary Care Physicians&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v088_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Dentists&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v062_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Mental Health Providers&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v005_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Preventable Hospital Stays&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v050_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Mammography Screening&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v155_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Flu Vaccinations&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v168_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;High School Completion&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v069_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Some College&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v023_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Unemployment&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v024_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Children in Poverty&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v044_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Income Inequality&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v082_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Children in Single-Parent Households&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v140_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Social Associations&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v135_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Injury Deaths&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v125_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Air Pollution - Particulate Matter&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v124_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Drinking Water Violations&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v136_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Severe Housing Problems&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v067_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Driving Alone to Work&quot;</span><span class="s1">,</span>
    <span class="s3">&quot;v137_rawvalue&quot;</span><span class="s1">: </span><span class="s3">&quot;Long Commute - Driving Alone&quot;</span>
<span class="s1">}</span>

<span class="s0"># Rename Columns</span>
<span class="s1">data = data.rename(columns=feature_mapping)</span>
<span class="s1">data.head()</span>
<span class="s0">#%% md 
</span>
<span class="s1">## 2. Exploratory Data Analysis (EDA) 
</span><span class="s0">#%% md 
</span><span class="s1">##### Handling Missing Data 
- Columns with more than **10% missing values** are dropped from the dataset. 
- For columns with **less than 10% missing values**, missing values are imputed using the **median** to preserve data distribution. 
 
##### Identifying Outliers 
- **Boxplots** are used to visualize potential outliers in each feature. 
- **Interquartile Range (IQR) method** is applied to detect and count the number of outliers. 
- Extreme values are handled using **Winsorization**, which scales back extreme data points while maintaining overall distribution. 
 
##### Computing Summary Statistics 
- The `data.describe()` function is used to compute key statistics: 
  - **Numerical features**: Includes **mean, median, standard deviation, min/max, and quartiles**. 
  - **Categorical features**: Includes **counts, unique values, and most frequent category** using `data.describe(include='object')`. 
</span><span class="s0">#%% md 
</span><span class="s1">### Handle missing data 
</span><span class="s0">#%% 
# Check for missing values</span>
<span class="s1">missing_values = data.isnull().sum()</span>
<span class="s1">missing_columns = missing_values[missing_values &gt; </span><span class="s4">0</span><span class="s1">].index</span>
<span class="s1">missing_columns</span>
<span class="s0">#%% md 
</span><span class="s1">Since no columns exceed the 10% missing threshold, we will impute all missing values using the median to avoid skewing the data. 
</span><span class="s0">#%% 
# Impute missing values using median</span>
<span class="s1">num_vars = data.select_dtypes(include=[np.number]).columns.tolist()</span>
<span class="s1">num_imputer = SimpleImputer(strategy=</span><span class="s3">'median'</span><span class="s1">)</span>
<span class="s1">data[num_vars] = num_imputer.fit_transform(data[num_vars])</span>
<span class="s1">print(</span><span class="s3">&quot;Remaining missing values after imputation:</span><span class="s5">\n</span><span class="s3">&quot;</span><span class="s1">, data.isnull().sum())</span>
<span class="s0">#%% md 
</span><span class="s1">### Identify outliers 
</span><span class="s0">#%% 
# Visualizing outliers using boxplots</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">15</span><span class="s1">, </span><span class="s4">8</span><span class="s1">))</span>
<span class="s1">data[num_vars].boxplot(rot=</span><span class="s4">90</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">&quot;Boxplot of Features to Identify Outliers&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% 
# Detect outliers using IQR</span>
<span class="s1">Q1 = data[num_vars].quantile(</span><span class="s4">0.25</span><span class="s1">)</span>
<span class="s1">Q3 = data[num_vars].quantile(</span><span class="s4">0.75</span><span class="s1">)</span>
<span class="s1">IQR = Q3 - Q1</span>
<span class="s1">outlier_mask = (data[num_vars] &lt; (Q1 - </span><span class="s4">1.5 </span><span class="s1">* IQR)) | (data[num_vars] &gt; (Q3 + </span><span class="s4">1.5 </span><span class="s1">* IQR))</span>
<span class="s1">outliers = outlier_mask.sum()</span>
<span class="s1">print(</span><span class="s3">&quot;Number of outliers per feature:</span><span class="s5">\n</span><span class="s3">&quot;</span><span class="s1">, outliers)</span>
<span class="s0">#%% md 
</span><span class="s1">**Applied Winsorization (Capping at 1st &amp; 99th Percentiles)** 
</span><span class="s0">#%% 
# Cap extreme values at 1st and 99th percentiles</span>
<span class="s2">for </span><span class="s1">col </span><span class="s2">in </span><span class="s1">num_vars:</span>
    <span class="s1">data[col] = winsorize(data[col], limits=[</span><span class="s4">0.01</span><span class="s1">, </span><span class="s4">0.01</span><span class="s1">])  </span><span class="s0"># Capping lowest and highest 1%</span>
<span class="s0">#%% md 
</span><span class="s1">### Computing Summary Statistics 
</span><span class="s0">#%% 
#Numerical columns</span>
<span class="s1">print(data.describe())</span>

<span class="s0">#prevents NumPy's &quot;MaskedArray&quot; partition warning from being printed</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s1">warnings.filterwarnings(</span><span class="s3">&quot;ignore&quot;</span><span class="s1">, category=UserWarning, module=</span><span class="s3">&quot;numpy.lib.function_base&quot;</span><span class="s1">)</span>
<span class="s0">#%% 
 # Categorical columns</span>
<span class="s1">print(data.describe(include=</span><span class="s3">'object'</span><span class="s1">)) </span>
<span class="s0">#%% md 
</span><span class="s1">## 3. Clustering 
</span><span class="s0">#%% md 
</span><span class="s1">I performed **K-Means clustering** for clustering and **PCA** for visualization, selecting the optimal number of clusters (k) based on the Elbow Method and Silhouette Score. The clustering was conducted using premature death rates and various health factors, to identify counties with similar health outcomes. 
</span><span class="s0">#%% 
</span><span class="s1">print(data.columns)</span>
<span class="s0">#%% 
# Selected features for clustering (All health factors)</span>
<span class="s1">selected_features = [</span>
    <span class="s3">'Adult Smoking'</span><span class="s1">, </span><span class="s3">'Adult Obesity'</span><span class="s1">, </span><span class="s3">'Food Environment Index'</span><span class="s1">, </span><span class="s3">'Physical Inactivity'</span><span class="s1">,</span>
    <span class="s3">'Access to Exercise Opportunities'</span><span class="s1">, </span><span class="s3">'Excessive Drinking'</span><span class="s1">, </span><span class="s3">'Alcohol-Impaired Driving Deaths'</span><span class="s1">,</span>
    <span class="s3">'Sexually Transmitted Infections'</span><span class="s1">, </span><span class="s3">'Teen Births'</span><span class="s1">, </span><span class="s3">'Uninsured'</span><span class="s1">, </span><span class="s3">'Primary Care Physicians'</span><span class="s1">, </span>
    <span class="s3">'Dentists'</span><span class="s1">, </span><span class="s3">'Mental Health Providers'</span><span class="s1">, </span><span class="s3">'Preventable Hospital Stays'</span><span class="s1">, </span><span class="s3">'Mammography Screening'</span><span class="s1">, </span>
    <span class="s3">'Flu Vaccinations'</span><span class="s1">, </span><span class="s3">'High School Completion'</span><span class="s1">, </span><span class="s3">'Some College'</span><span class="s1">, </span><span class="s3">'Unemployment'</span><span class="s1">, </span>
    <span class="s3">'Children in Poverty'</span><span class="s1">, </span><span class="s3">'Income Inequality'</span><span class="s1">, </span><span class="s3">'Children in Single-Parent Households'</span><span class="s1">, </span>
    <span class="s3">'Social Associations'</span><span class="s1">, </span><span class="s3">'Injury Deaths'</span><span class="s1">, </span><span class="s3">'Air Pollution - Particulate Matter'</span><span class="s1">, </span>
    <span class="s3">'Drinking Water Violations'</span><span class="s1">, </span><span class="s3">'Severe Housing Problems'</span><span class="s1">, </span><span class="s3">'Driving Alone to Work'</span><span class="s1">, </span>
    <span class="s3">'Long Commute - Driving Alone'</span>
<span class="s1">]</span>

<span class="s0"># Ensure all selected features exist in the dataset</span>
<span class="s1">available_features = [col </span><span class="s2">for </span><span class="s1">col </span><span class="s2">in </span><span class="s1">selected_features </span><span class="s2">if </span><span class="s1">col </span><span class="s2">in </span><span class="s1">data.columns]</span>

<span class="s0"># Standardize features before clustering</span>
<span class="s1">scaler = StandardScaler()</span>
<span class="s1">X_scaled = scaler.fit_transform(data[selected_features])</span>

<span class="s0"># Determine the optimal number of clusters using Elbow Method &amp; Silhouette Score</span>
<span class="s1">inertia = []</span>
<span class="s1">silhouette_scores = []</span>
<span class="s1">cluster_range = range(</span><span class="s4">2</span><span class="s1">, </span><span class="s4">10</span><span class="s1">)</span>

<span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">cluster_range:</span>
    <span class="s1">kmeans = KMeans(n_clusters=k, random_state=</span><span class="s4">42</span><span class="s1">, n_init=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">kmeans.fit(X_scaled)</span>
    <span class="s1">inertia.append(kmeans.inertia_)</span>
    <span class="s1">silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))</span>

<span class="s0"># Plot Elbow Method</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">8</span><span class="s1">, </span><span class="s4">4</span><span class="s1">))</span>
<span class="s1">plt.plot(cluster_range, inertia, marker=</span><span class="s3">'o'</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">&quot;Number of Clusters&quot;</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">&quot;Inertia&quot;</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">&quot;Elbow Method for Optimal Clusters (Scaled Data)&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% 
# Plot Silhouette Score</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">8</span><span class="s1">, </span><span class="s4">4</span><span class="s1">))</span>
<span class="s1">plt.plot(cluster_range, silhouette_scores, marker=</span><span class="s3">'o'</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">&quot;Number of Clusters&quot;</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">&quot;Silhouette Score&quot;</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">&quot;Silhouette Score for Different Clusters (Scaled Data)&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">To determine the optimal number of clusters, I used two methods: 
 
1. **Elbow Method**   
- The elbow point in the inertia plot bends slightly at **k=4 or k=5** 
- Beyond k=5, the decrease in inertia slows down, meaning additional clusters do not provide much new information. 
 
2. **Silhouette Score Analysis**   
- The silhouette score is highest at **k=2**, but **k=4** maintains a good balance of separation between clusters. 
- The silhouette score drops sharply after k=3, and remains low beyond k=4 or k=5, indicating that clusters become less distinct. 
 
#### Conclusion: 
- **k=4 offers the best trade-off** between meaningful structure (Elbow Method) and cluster quality (Silhouette Score). 
 
</span><span class="s0">#%% 
# Choosing the optimal number of clusters based on the elbow method and silhouette score</span>
<span class="s1">optimal_k = </span><span class="s4">4</span>
<span class="s1">kmeans = KMeans(n_clusters=optimal_k, random_state=</span><span class="s4">42</span><span class="s1">, n_init=</span><span class="s4">10</span><span class="s1">)</span>
<span class="s1">data[</span><span class="s3">'Cluster'</span><span class="s1">] = kmeans.fit_predict(X_scaled)</span>
<span class="s0">#%% 
# Reduce to 2 principal components for visualization</span>
<span class="s1">pca = PCA(n_components=</span><span class="s4">2</span><span class="s1">)</span>
<span class="s1">X_pca = pca.fit_transform(X_scaled)</span>

<span class="s0"># Add PCA results to DataFrame</span>
<span class="s1">data[</span><span class="s3">&quot;PCA1&quot;</span><span class="s1">] = X_pca[:, </span><span class="s4">0</span><span class="s1">]</span>
<span class="s1">data[</span><span class="s3">&quot;PCA2&quot;</span><span class="s1">] = X_pca[:, </span><span class="s4">1</span><span class="s1">]</span>

<span class="s0"># Scatter plot using PCA components</span>
<span class="s1">sns.scatterplot(x=data[</span><span class="s3">&quot;PCA1&quot;</span><span class="s1">], y=data[</span><span class="s3">&quot;PCA2&quot;</span><span class="s1">], hue=data[</span><span class="s3">&quot;Cluster&quot;</span><span class="s1">], palette=</span><span class="s3">&quot;viridis&quot;</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">&quot;Cluster Visualization using PCA&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% 
# Add premature date to the features for calculation</span>
<span class="s1">features_to_profile = available_features + [</span><span class="s3">'Premature Death'</span><span class="s1">] </span><span class="s2">if </span><span class="s3">'Premature Death' </span><span class="s2">in </span><span class="s1">data.columns </span><span class="s2">else </span><span class="s1">available_features</span>

<span class="s0"># Compute the mean value of each feature for each cluster</span>
<span class="s1">cluster_means = data.groupby(</span><span class="s3">&quot;Cluster&quot;</span><span class="s1">)[features_to_profile].mean()</span>

<span class="s0"># Create a Ranking Table</span>

<span class="s0"># For each feature (column), rank the clusters based on the average value.</span>
<span class="s1">ranking_table = cluster_means.rank(axis=</span><span class="s4">0</span><span class="s1">, ascending=</span><span class="s2">False</span><span class="s1">)</span>
<span class="s1">ranking_table = ranking_table.astype(int)  </span><span class="s0"># Convert to integer rankings for clarity</span>

<span class="s1">print(</span><span class="s3">&quot;</span><span class="s5">\n</span><span class="s3">Ranking Table (each cell shows the rank of the cluster for that feature; 1 = highest average):&quot;</span><span class="s1">)</span>
<span class="s1">print(ranking_table)</span>
<span class="s2">try</span><span class="s1">:</span>
    <span class="s2">from </span><span class="s1">IPython.display </span><span class="s2">import </span><span class="s1">display</span>
    <span class="s1">display(ranking_table.style.background_gradient(cmap=</span><span class="s3">'viridis'</span><span class="s1">))</span>
<span class="s2">except </span><span class="s1">ImportError:</span>
    <span class="s1">print(ranking_table)</span>
<span class="s0">#%% 
# Premature death of each cluster</span>
<span class="s2">if </span><span class="s3">'Premature Death' </span><span class="s2">in </span><span class="s1">data.columns:</span>
    <span class="s1">outcome_summary = data.groupby(</span><span class="s3">&quot;Cluster&quot;</span><span class="s1">)[</span><span class="s3">&quot;Premature Death&quot;</span><span class="s1">].mean()</span>
    <span class="s1">print(</span><span class="s3">&quot;</span><span class="s5">\n</span><span class="s3">Average Premature Death by Cluster:&quot;</span><span class="s1">)</span>
    <span class="s1">print(outcome_summary)</span>
<span class="s0">#%% md 
</span><span class="s1">### Analysis:  
#### Cluster 1: Worst Health Outcomes   
Cluster 1 has the highest premature death rate at **14,299 per 100,000 population**. This cluster has the highest levels of **obesity, smoking, and physical inactivity**. Healthcare access is poor, with fewer primary care physicians and dentists. Socioeconomic conditions are also challenging, with **high unemployment, poverty, and income inequality**. Additionally, this cluster has a high number of preventable hospital stays, suggesting a lack of preventive care and routine medical check-ups. 
 
#### Cluster 2: Above-Average Health Outcomes 
Cluster 2 has a **premature death rate of 10,984 per 100,000 population**, which is above average but lower than Cluster 1. It also has the **second-highest smoking and obesity rates**. Healthcare access is more limited compared to Clusters 0 and 3, making it harder for people to manage their health. This cluster has **moderate levels of poverty and income inequality**, which may contribute to worse health behaviors and outcomes. 
 
#### Cluster 0: Moderate Health Outcomes   
Cluster 0 has a **premature death rate of 8,025 per 100,000 population**. Health behaviors in this cluster are **moderate**, with mid-range levels of **obesity, smoking, and healthcare access**. Socioeconomic conditions, including **income inequality and poverty**, are also moderate. Compared to Cluster 3, this group has slightly worse health behaviors but better conditions than Clusters 1 and 2. 
 
#### Cluster 3: Best Health Outcomes   
Cluster 3 has the **lowest premature death rate at 7,409 per 100,000 population**. This cluster has **better healthcare access**, with more primary care physicians, dentists, and mental health providers. It also has **lower levels of smoking, obesity, and physical inactivity**, contributing to better health outcomes. **Higher education levels** and **better socioeconomic conditions** help explain the healthier behaviors and lower death rates in this group. 
 
### Summary   
Clusters 1 and 2 have higher premature death rates, worse health behaviors, and more limited healthcare access. Clusters 0 and 3, on the other hand, have better healthcare availability, lower smoking and obesity rates, and stronger socioeconomic conditions. Addressing healthcare gaps and promoting healthier behaviors in Clusters 1 and 2 could help improve overall health outcomes. 
 
 
 
 
 
</span><span class="s0">#%% 
</span><span class="s2">import </span><span class="s1">geopandas </span><span class="s2">as </span><span class="s1">gpd</span>
<span class="s2">import </span><span class="s1">matplotlib.colors </span><span class="s2">as </span><span class="s1">mcolors</span>
<span class="s2">import </span><span class="s1">matplotlib.patches </span><span class="s2">as </span><span class="s1">mpatches</span>

<span class="s1">shapefile_path = </span><span class="s3">'/Users/mai/Desktop/intro ai/hw1/hw1/data/UScounties' </span>

<span class="s0"># Load the U.S. counties shapefile</span>
<span class="s1">us_counties = gpd.read_file(shapefile_path)</span>

<span class="s0"># Ensure the FIPS codes are strings for merging</span>
<span class="s1">us_counties[</span><span class="s3">'FIPS'</span><span class="s1">] = us_counties[</span><span class="s3">'FIPS'</span><span class="s1">].astype(str)</span>

<span class="s0"># Ensure 'fipscode' column is also a string for merging</span>
<span class="s1">data[</span><span class="s3">'fipscode'</span><span class="s1">] = data[</span><span class="s3">'fipscode'</span><span class="s1">].astype(str)</span>

<span class="s0"># Remove Alaska (FIPS starts with '02') and Hawaii (FIPS starts with '15')</span>
<span class="s1">map_data = us_counties[~us_counties[</span><span class="s3">&quot;FIPS&quot;</span><span class="s1">].str.startswith((</span><span class="s3">&quot;02&quot;</span><span class="s1">, </span><span class="s3">&quot;15&quot;</span><span class="s1">))]</span>

<span class="s0"># Merge the shapefile with clustering data</span>
<span class="s1">map_data = map_data.merge(data[[</span><span class="s3">'fipscode'</span><span class="s1">, </span><span class="s3">'Cluster'</span><span class="s1">]], left_on=</span><span class="s3">'FIPS'</span><span class="s1">, right_on=</span><span class="s3">'fipscode'</span><span class="s1">, how=</span><span class="s3">'left'</span><span class="s1">)</span>

<span class="s0"># Convert to Albers Equal Area projection (for better scaling)</span>
<span class="s1">map_data = map_data.to_crs(epsg=</span><span class="s4">5070</span><span class="s1">)</span>

<span class="s0"># Define the colormap</span>
<span class="s1">cmap = plt.get_cmap(</span><span class="s3">'viridis'</span><span class="s1">)</span>
<span class="s1">norm = mcolors.Normalize(vmin=map_data[</span><span class="s3">'Cluster'</span><span class="s1">].min(), vmax=map_data[</span><span class="s3">'Cluster'</span><span class="s1">].max())</span>

<span class="s0"># Create a legend mapping cluster numbers to their colors</span>
<span class="s1">legend_patches = [</span>
    <span class="s1">mpatches.Patch(color=cmap(norm(cluster)), label=</span><span class="s3">f'Cluster </span><span class="s5">{</span><span class="s1">cluster</span><span class="s5">}</span><span class="s3">'</span><span class="s1">)</span>
    <span class="s2">for </span><span class="s1">cluster </span><span class="s2">in </span><span class="s1">sorted(map_data[</span><span class="s3">'Cluster'</span><span class="s1">].unique())</span>
<span class="s1">]</span>

<span class="s0"># Plot the clusters</span>
<span class="s1">fig, ax = plt.subplots(figsize=(</span><span class="s4">15</span><span class="s1">, </span><span class="s4">10</span><span class="s1">))</span>
<span class="s1">map_data.boundary.plot(ax=ax, linewidth=</span><span class="s4">0.8</span><span class="s1">, color=</span><span class="s3">&quot;black&quot;</span><span class="s1">)</span>
<span class="s1">map_data.plot(column=</span><span class="s3">'Cluster'</span><span class="s1">, cmap=cmap, ax=ax, legend=</span><span class="s2">True</span><span class="s1">,</span>
              <span class="s1">legend_kwds={</span><span class="s3">'label'</span><span class="s1">: </span><span class="s3">&quot;Cluster Assignment&quot;</span><span class="s1">, </span><span class="s3">'orientation'</span><span class="s1">: </span><span class="s3">&quot;horizontal&quot;</span><span class="s1">})</span>

<span class="s0"># Add legend with cluster-color mapping</span>
<span class="s1">plt.legend(handles=legend_patches, title=</span><span class="s3">&quot;Cluster Colors&quot;</span><span class="s1">, loc=</span><span class="s3">&quot;lower right&quot;</span><span class="s1">, fontsize=</span><span class="s4">10</span><span class="s1">, frameon=</span><span class="s2">True</span><span class="s1">)</span>

<span class="s0"># Set title and remove axis labels</span>
<span class="s1">plt.title(</span><span class="s3">'Clusters of U.S. Counties Based on Health Outcomes (48 States)'</span><span class="s1">, fontsize=</span><span class="s4">14</span><span class="s1">)</span>
<span class="s1">plt.axis(</span><span class="s3">'off'</span><span class="s1">)</span>

<span class="s0"># Show the map</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">### Analysis 
#### **Cluster 1 (Worst Health Outcomes)** 
Found mostly in the **Southeast and Appalachian regions** 
 
#### **Cluster 2 (Above-Average Health Risks)** 
Scattered across parts of the **Midwest and some urban areas** 
 
#### **Cluster 0 (Moderate Health Outcomes)** 
Spread across the **Great Plains and parts of the East Coast** 
 
#### **Cluster 3 (Best Health Outcomes)** 
Primarily located in **coastal and urban areas**, including the **Northeast and West Coast** 
 
#### **Conclusion** 
The geographic distribution of clusters highlights **significant health disparities across U.S. regions**, emphasizing the need for **targeted healthcare interventions** in high-risk areas like the **Southeast and rural Midwest**. 
 
</span><span class="s0">#%% md 
</span><span class="s1">## 4. Supervised Learning Models 
</span><span class="s0">#%% md 
</span><span class="s1">For this analysis, I applied **Linear Regression** and **Random Forest** to predict premature death using various health behavior indicators as input features. These features include factors such as smoking rates, obesity, access to healthcare, physical inactivity, and socioeconomic conditions. 
 
Both models were evaluated based on their **Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R² Score** to determine their accuracy in predicting premature death.  
</span><span class="s0">#%% 
# Selecting features and target variable for supervised learning</span>
<span class="s1">target = </span><span class="s3">'Premature Death'</span>
<span class="s1">X = data[available_features]</span>
<span class="s1">y = data[target]</span>

<span class="s0"># Split data into 80% training and 20% test</span>
<span class="s1">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=</span><span class="s4">0.2</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>

<span class="s0"># Standardize the features</span>
<span class="s1">scaler = StandardScaler()</span>
<span class="s1">X_train_scaled = scaler.fit_transform(X_train)</span>
<span class="s1">X_test_scaled = scaler.transform(X_test)</span>
<span class="s0">#%% 
# Cross-validation</span>
<span class="s0"># Number of folds</span>
<span class="s1">cv = </span><span class="s4">5 </span>

<span class="s0"># Linear Regression Cross-Validation</span>
<span class="s1">lr_model = LinearRegression()</span>
<span class="s1">lr_cv_mse = cross_val_score(lr_model, X_train_scaled, y_train, cv=cv, scoring=</span><span class="s3">'neg_mean_squared_error'</span><span class="s1">)</span>
<span class="s1">lr_cv_rmse = np.sqrt(-lr_cv_mse)</span>
<span class="s1">print(</span><span class="s3">&quot;Linear Regression Cross-Validation RMSE scores:&quot;</span><span class="s1">, lr_cv_rmse)</span>
<span class="s1">print(</span><span class="s3">&quot;Mean Linear Regression CV RMSE:&quot;</span><span class="s1">, lr_cv_rmse.mean())</span>

<span class="s0"># Random Forest Cross-Validation</span>
<span class="s1">rf_model = RandomForestRegressor(n_estimators=</span><span class="s4">100</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">rf_cv_mse = cross_val_score(rf_model, X_train_scaled, y_train, cv=cv, scoring=</span><span class="s3">'neg_mean_squared_error'</span><span class="s1">)</span>
<span class="s1">rf_cv_rmse = np.sqrt(-rf_cv_mse)</span>
<span class="s1">print(</span><span class="s3">&quot;</span><span class="s5">\n</span><span class="s3">Random Forest Cross-Validation RMSE scores:&quot;</span><span class="s1">, rf_cv_rmse)</span>
<span class="s1">print(</span><span class="s3">&quot;Mean Random Forest CV RMSE:&quot;</span><span class="s1">, rf_cv_rmse.mean())</span>
<span class="s0">#%% 
# Train the final Linear Regression model</span>
<span class="s1">lr_model.fit(X_train_scaled, y_train)</span>
<span class="s0"># Train the final Random Forest model</span>
<span class="s1">rf_model.fit(X_train_scaled, y_train)</span>

<span class="s0"># Predictions on the test set</span>
<span class="s1">y_pred_lr = lr_model.predict(X_test_scaled)</span>
<span class="s1">y_pred_rf = rf_model.predict(X_test_scaled)</span>

<span class="s0"># Compute performance metrics for Linear Regression</span>
<span class="s1">lr_mse = mean_squared_error(y_test, y_pred_lr)</span>
<span class="s1">lr_rmse = np.sqrt(lr_mse)</span>
<span class="s1">lr_r2 = r2_score(y_test, y_pred_lr)</span>

<span class="s0"># Compute performance metrics for Random Forest</span>
<span class="s1">rf_mse = mean_squared_error(y_test, y_pred_rf)</span>
<span class="s1">rf_rmse = np.sqrt(rf_mse)</span>
<span class="s1">rf_r2 = r2_score(y_test, y_pred_rf)</span>

<span class="s0"># Display performance metrics</span>
<span class="s1">model_performance = pd.DataFrame({</span>
    <span class="s3">&quot;Model&quot;</span><span class="s1">: [</span><span class="s3">&quot;Linear Regression&quot;</span><span class="s1">, </span><span class="s3">&quot;Random Forest&quot;</span><span class="s1">],</span>
    <span class="s3">&quot;MSE&quot;</span><span class="s1">: [lr_mse, rf_mse],</span>
    <span class="s3">&quot;RMSE&quot;</span><span class="s1">: [lr_rmse, rf_rmse],</span>
    <span class="s3">&quot;R² Score&quot;</span><span class="s1">: [lr_r2, rf_r2]</span>
<span class="s1">})</span>
<span class="s1">print(</span><span class="s3">&quot;</span><span class="s5">\n</span><span class="s3">Test Set Performance:&quot;</span><span class="s1">)</span>
<span class="s1">print(model_performance)</span>
<span class="s0">#%% 
# For Linear Regression: Top 5 features by absolute coefficient value</span>
<span class="s1">lr_coef_df = pd.DataFrame({</span>
    <span class="s3">'Feature'</span><span class="s1">: available_features,</span>
    <span class="s3">'Coefficient'</span><span class="s1">: lr_model.coef_</span>
<span class="s1">})</span>
<span class="s1">lr_coef_df[</span><span class="s3">'AbsCoefficient'</span><span class="s1">] = lr_coef_df[</span><span class="s3">'Coefficient'</span><span class="s1">].abs()</span>
<span class="s1">top5_lr = lr_coef_df.sort_values(by=</span><span class="s3">'AbsCoefficient'</span><span class="s1">, ascending=</span><span class="s2">False</span><span class="s1">).head(</span><span class="s4">5</span><span class="s1">)</span>
<span class="s1">print(</span><span class="s3">&quot;</span><span class="s5">\n</span><span class="s3">Top 5 features influencing premature death (Linear Regression):&quot;</span><span class="s1">)</span>
<span class="s1">print(top5_lr[[</span><span class="s3">'Feature'</span><span class="s1">, </span><span class="s3">'Coefficient'</span><span class="s1">]])</span>
<span class="s0">#%% 
# For Random Forest: Top 5 features by feature importance</span>
<span class="s1">rf_importance_df = pd.DataFrame({</span>
    <span class="s3">'Feature'</span><span class="s1">: available_features,</span>
    <span class="s3">'Importance'</span><span class="s1">: rf_model.feature_importances_</span>
<span class="s1">})</span>
<span class="s1">top5_rf = rf_importance_df.sort_values(by=</span><span class="s3">'Importance'</span><span class="s1">, ascending=</span><span class="s2">False</span><span class="s1">).head(</span><span class="s4">5</span><span class="s1">)</span>
<span class="s1">print(</span><span class="s3">&quot;</span><span class="s5">\n</span><span class="s3">Top 5 features influencing premature death (Random Forest):&quot;</span><span class="s1">)</span>
<span class="s1">print(top5_rf)</span>
<span class="s0">#%% md 
</span><span class="s1">#### **Comparison of 2 models:** 
The performance difference is extremely small, but Linear Regression performs slightly better in all three metrics (lower MSE, lower RMSE, and higher R²). Thus, Linear Regression is preferable because it provides better interpretability, making it easier to understand the impact of each predictor on premature death. Random Forest is typically more robust for capturing non-linear relationships, but in this case, it does not offer a significant advantage. 
 
 
</span><span class="s0">#%% md 
</span><span class="s1">## 5. Recommendations to reduce premature death 
</span><span class="s0">#%% md 
</span><span class="s1">#### Cluster of Allegheny County 
</span><span class="s0">#%% 
</span><span class="s1">allegheny_cluster = data[data[</span><span class="s3">'county'</span><span class="s1">].str.contains(</span><span class="s3">&quot;Allegheny&quot;</span><span class="s1">, case=</span><span class="s2">False</span><span class="s1">, na=</span><span class="s2">False</span><span class="s1">)]</span>
<span class="s1">print(allegheny_cluster[[</span><span class="s3">'county'</span><span class="s1">, </span><span class="s3">'state'</span><span class="s1">, </span><span class="s3">'Cluster'</span><span class="s1">]])</span>
<span class="s0">#%% md 
</span><span class="s1">Since Allegheny County belongs to Cluster 3, which has the best health outcomes, the focus should be on maintaining progress, addressing remaining disparities, and enhancing preventive care. In addition, based on the top 5 features identified by the Linear Regression and Random Forest models, key areas for intervention include **injury prevention, reducing smoking, addressing child poverty, improving sexual health, and lowering teen birth rates.** 
 
#### Immediate Strategies 
 
**Expand Preventive Care Programs:** Encourage regular health screenings, vaccinations, and chronic disease management to reduce preventable hospital visits and improve long-term health outcomes. 
 
**Strengthen Injury Prevention:** Enforce stricter DUI laws and road safety improvements and provide workplace safety training in high-risk industries. 
 
**Enhance Smoking Cessation:** Strengthen anti-smoking campaigns, targeting youth and low-income populations. 
 
**Improve Sexual &amp; Reproductive Health:** Increase access to contraception, STI screenings, and education in clinics and schools. Expand teen pregnancy prevention programs in at-risk communities. 
 
**Enhance Workplace Wellness Initiatives:** Encourage employers to offer wellness programs, such as gym memberships, mental health support, and nutrition counseling, to keep the workforce healthy. 
 
#### Long-term Strategies 
 
**Maintain Strong Healthcare Access:** Allegheny County already has a high number of primary care physicians, dentists, and mental health providers. Efforts should focus on ensuring equitable access so all residents, including lower-income populations, can benefit. 
 
**Promote Health Equity:** Even in well-performing counties, disparities exist. Target low-income neighborhoods and underserved populations with programs that provide affordable healthcare, healthy food access, and fitness opportunities. 
 
**Invest in Data-Driven Health Improvements:** Use county-level health data to target specific neighborhoods with higher smoking, obesity, and substance use rates and tailor intervention programs. 
 
**Reduce Socioeconomic Barriers** Strengthen childhood poverty reduction programs like affordable childcare and early education. 
</span><span class="s0">#%% md 
</span><span class="s1">## 6. External Libraries and References 
</span><span class="s0">#%% md 
</span><span class="s1">- **Scikit-learn Developers.** (n.d.). *SimpleImputer (sklearn.impute.SimpleImputer)*. Retrieved from [https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) 
 
- **SciPy Developers.** (n.d.). *Winsorization (scipy.stats.mstats.winsorize)*. Retrieved from [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.winsorize.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.winsorize.html) 
 
- **Analytics Vidhya.** (2021). *K-Means: Getting the Optimal Number of Clusters*. Retrieved from [https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters](https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters) 
 
- **IPython Developers.** (n.d.). *IPython Display Module*. Retrieved from [https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html) 
 
- **J. Cutrer.** (n.d.). *Learn GeoPandas: Plotting US Maps in Python*. Retrieved from [https://jcutrer.com/python/learn-geopandas-plotting-usmaps](https://jcutrer.com/python/learn-geopandas-plotting-usmaps) 
 
- **Ruiz, J. L.** (n.d.). *Plot Maps from the US Census Bureau Using GeoPandas and Contextily in Python*. Retrieved from [https://medium.com/@jl_ruiz/plot-maps-from-the-us-census-bureau-using-geopandas-and-contextily-in-python-df787647ef77](https://medium.com/@jl_ruiz/plot-maps-from-the-us-census-bureau-using-geopandas-and-contextily-in-python-df787647ef77) 
 
- **Scikit-learn Developers.** (n.d.). *Forest of trees-based ensemble methods (Random Forest)*. In *scikit-learn: Machine Learning in Python*. Retrieved from [https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py) 
 
- **Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., &amp; Duchesnay, E.** (2011). *Scikit-learn: Machine Learning in Python.* *Journal of Machine Learning Research*, 12, 2825–2830. Retrieved from [https://jmlr.org/papers/v12/pedregosa11a.html](https://jmlr.org/papers/v12/pedregosa11a.html) 
</span></pre>
</body>
</html>
